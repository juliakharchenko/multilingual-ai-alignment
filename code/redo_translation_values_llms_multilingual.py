# -*- coding: utf-8 -*-
"""redo-translation-values-llms-multilingual.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11LW6f3MXhN4JENK0fBOfuIVXq_YQRq0Q
"""

# # Use a pipeline as a high-level helper
# from transformers import pipeline

# pipe = pipeline("translation", model="facebook/nllb-200-418M")

# Load model directly
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, M2M100ForConditionalGeneration


model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M")
tokenizer = AutoTokenizer.from_pretrained("facebook/m2m100_418M")

print(tokenizer)

tokenizer.src_lang = "en"
hi_text = "Every human deserves rights"
encoded_hi = tokenizer(hi_text, return_tensors="pt")
generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id("uk"))
tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)

# pip install openai

# from google.colab import drive
# drive.mount('/content/drive')

#pip install paraphraser

import nltk
nltk.download('averaged_perceptron_tagger')
nltk.download('punkt')
nltk.download('wordnet')
from nltk.tokenize import sent_tokenize

# pip install git+https://github.com/PrithivirajDamodaran/Parrot_Paraphraser.git

pip install rouge

import pandas as pd
from nltk.translate.bleu_score import SmoothingFunction
from nltk.tokenize import word_tokenize
from nltk.corpus import wordnet
from nltk.tag import pos_tag
import time
from rouge import Rouge
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
from google.colab import files

# translator = pipeline("translation", model="facebook/nllb-200-3.3B")
# tokenizer = AutoTokenizer.from_pretrained("facebook/nllb-200-3.3B")
# model = AutoModelForSeq2SeqLM.from_pretrained("facebook/nllb-200-3.3B")

def translate_text(text, target_language):
    try:
        # Tokenize the input text into sentences
        sentences = sent_tokenize(text)

        # Initialize an empty list to store translated sentences
        translated_sentences = []

        # Iterate over each sentence, translate, and append to the list
        for sentence in sentences:
            print(target_language)
            tokenizer.src_lang = "en"
            encoded_text = tokenizer(sentence, return_tensors="pt")
            generated_tokens = model.generate(**encoded_text, forced_bos_token_id=tokenizer.get_lang_id(target_language))
            translated_sentence = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]
            translated_sentences.append(translated_sentence)

        # Combine the translated sentences into a single translated text
        translated_text = ' '.join(translated_sentences)

        return translated_text
    except Exception as e:
        print(f"Translation failed: {e}")
        return None


def back_translate_text(text, src_language):
    try:

        # Tokenize the input text into sentences
        sentences = sent_tokenize(text)

        # Initialize an empty list to store translated sentences
        translated_sentences = []

        # Iterate over each sentence, translate, and append to the list
        for sentence in sentences:
            print(src_language)
            tokenizer.src_lang = src_language
            encoded_text = tokenizer(sentence, return_tensors="pt")
            generated_tokens = model.generate(**encoded_text, forced_bos_token_id=tokenizer.get_lang_id("en"))
            translated_sentence = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]
            translated_sentences.append(translated_sentence)

        # Combine the translated sentences into a single translated text
        translated_text = ' '.join(translated_sentences)

        return translated_text
    except Exception as e:
        print(f"Translation failed: {e}")
        return None

def calculate_rouge_score(reference, candidate):
    rouge = Rouge()
    scores = rouge.get_scores(candidate, reference)
    return scores[0]['rouge-l']['f']

def generate_paraphrases(sentence, num_paraphrases=1):
    paraphrases = set()
    tokens = word_tokenize(sentence)
    tagged_tokens = pos_tag(tokens)

    pos_groups = {}
    for token, tag in tagged_tokens:
        if tag in pos_groups:
            pos_groups[tag].append(token)
        else:
            pos_groups[tag] = [token]

    for tag, tokens_group in pos_groups.items():
        if tag.startswith('NN') or tag.startswith('VB') or tag.startswith('JJ'):
            synonyms_group = set()
            for token in tokens_group:
                for syn in wordnet.synsets(token):
                    for lemma in syn.lemmas():
                        synonym = lemma.name().replace("_", " ")
                        if lemma.name() != token and wordnet.synsets(synonym):
                            similarity_threshold = 0.7
                            if syn.path_similarity(wordnet.synsets(token)[0]) is not None and syn.path_similarity(wordnet.synsets(synonym)[0]) is not None:
                                if syn.path_similarity(wordnet.synsets(token)[0]) > similarity_threshold and syn.path_similarity(wordnet.synsets(synonym)[0]) > similarity_threshold:
                                    synonyms_group.add(synonym)

            if synonyms_group:
                for _ in range(min(num_paraphrases, len(synonyms_group))):
                    paraphrase_tokens = tokens[:]
                    for original_token, synonym in zip(tokens_group, synonyms_group):
                        for i, token in enumerate(p for p in paraphrase_tokens if p == original_token):
                            paraphrase_tokens[paraphrase_tokens.index(token)] = synonym
                    paraphrase = ' '.join(paraphrase_tokens)
                    if paraphrase != sentence:
                        paraphrases.add(paraphrase)
                    if len(paraphrases) >= num_paraphrases:
                        break
        if len(paraphrases) >= num_paraphrases:
            break
    return list(paraphrases)[:num_paraphrases]

def translate_csv(input_csv, output_csv, target_languages, num):
    df = pd.read_csv(input_csv)

    rows = []

    for index, row in df.iterrows():
        for lang in target_languages:
            print(row['English_Prompt'])
            translated_prompt = translate_text(row['English_Prompt'], lang)
            print(translated_prompt)
            back_translated_prompt = back_translate_text(translated_prompt, lang)
            print(back_translated_prompt)
            rouge_score = calculate_rouge_score(row['English_Prompt'], back_translated_prompt)
            print(rouge_score)
            if num == 0:
              rows.append([row['English_Prompt'], translated_prompt,
                           back_translated_prompt, rouge_score, lang,
                           row['Expected_Individualistic'], row['Expected_Collectivist']])
            elif num == 1:
              rows.append([row['English_Prompt'], translated_prompt,
                           back_translated_prompt, rouge_score, lang,
                           row['Expected_Long_Term'], row['Expected_Short_Term']])
            elif num == 2:
              rows.append([row['English_Prompt'], translated_prompt,
                           back_translated_prompt, rouge_score, lang,
                           row['Expected_Masculine'], row['Expected_Feminine']])
            elif num == 3:
              rows.append([row['English_Prompt'], translated_prompt,
                           back_translated_prompt, rouge_score, lang,
                           row['Expected_High_Power_Distance'], row['Expected_Low_Power_Distance']])
            else:
              rows.append([row['English_Prompt'], translated_prompt,
                           back_translated_prompt, rouge_score, lang,
                           row['Expected_High_Uncertainty_Avoidance'], row['Expected_Low_Uncertainty_Avoidance']])
    # this need to change based on the input and output that we are using as well, so we can take in
    # multiple different files to translate
    if num == 0:
       translated_df = pd.DataFrame(rows, columns=['English_Prompt',
                      'Translated', 'Back_Translated', 'ROUGE_Score',
                      'Target_Language', 'Expected_Individualistic', 'Expected_Collectivist'])
    elif num == 1:
      translated_df = pd.DataFrame(rows, columns=['English_Prompt',
                      'Translated', 'Back_Translated', 'ROUGE_Score',
                      'Target_Language', 'Expected_Long_Term', 'Expected_Short_Term'])
    elif num == 2:
      translated_df = pd.DataFrame(rows, columns=['English_Prompt',
                      'Translated', 'Back_Translated', 'ROUGE_Score',
                      'Target_Language', 'Expected_Masculine', 'Expected_Feminine'])
    elif num == 3:
      translated_df = pd.DataFrame(rows, columns=['English_Prompt', 'Translated',
                                                  'Back_Translated', 'ROUGE_Score',
                      'Target_Language',
                      'Expected_High_Power_Distance', 'Expected_Low_Power_Distance'])
    else:
      translated_df = pd.DataFrame(rows, columns=['English_Prompt', 'Translated',
                                                  'Back_Translated', 'ROUGE_Score',
                      'Target_Language',
                      'Expected_High_Uncertainty_Avoidance', 'Expected_Low_Uncertainty_Avoidance'])

    translated_df.to_csv(output_csv, index=False)
    # files.download(output_csv)

# Individualistic vs Collectivist
multilingual_individualistic_vs_collectivist_df = pd.read_csv("../data/individualistic_vs_collectivist.csv")
output_multilingual_individualistic_vs_collectivist_csv = "output_multilingual_individualistic_collectivist.csv"

# Long term vs short term orientation
multilingual_orientation_df = pd.read_csv("../data/long_term_vs_short_term_orientation.csv")
output_multilingual_orientation_csv = "output_multilingual_orientation.csv"

# Masculinity vs femininity
multilingual_mas_df = pd.read_csv("../data/masculinity_femininity.csv")
output_multilingual_mas_csv = "output_multilingual_mas.csv"

# Power distance index
multilingual_power_distance_df = pd.read_csv("../data/power_distance_index.csv")
output_multilingual_power_distance_csv = "output_multilingual_power_distance.csv"

# Uncertainty avoidance
multilingual_uncertainty_avoidance_df = pd.read_csv("../data/uncertainty_avoidance.csv")
output_multilingual_uncertainity_avoidance_csv = "output_multilingual_uncertainty.csv"

datasets = [
    multilingual_individualistic_vs_collectivist_df,
  multilingual_orientation_df,
  multilingual_mas_df,
  multilingual_power_distance_df,
  multilingual_uncertainty_avoidance_df
    ]

outputs = [
  output_multilingual_individualistic_vs_collectivist_csv ,
  output_multilingual_orientation_csv,
  output_multilingual_mas_csv,
  output_multilingual_power_distance_csv,
  output_multilingual_uncertainity_avoidance_csv
]

target_languages = [
        # "en",  # English
        "de",  # German
        "ru",  # Russian
        "ja",  # Japanese
        "fr",  # French
        "it",  # Italian
        "zh",  # Chinese
        "id",  # Indonesian
        "tr",  # Turkish
        "nl",  # Dutch
        "pl",  # Polish
        "fa",  # Persian
        "ko",  # Korean
        "cs",  # Czech
        "uk",  # Ukrainian
        "hu",  # Hungarian
        "el",  # Greek
        "ro",  # Romanian
        "sv",  # Swedish
        "he",  # Hebrew
        "da",  # Danish
        "th",  # Thai
        "fi",  # Finnish
        "bg",  # Bulgarian
        "kk",  # Kazakh
        "hy",  # Armenian
        "ka",  # Georgian
        "sq",  # Albanian
        "az",  # Azerbaijani
        "ms",  # Malay
        "mn",  # Mongolian
        "be",  # Belarusian
        "hi",  # Hindi
        "af",  # Afrikaans
        "is",  # Icelandic
        "si"   # Sinhala
    ]

# Going through each file
num = 0
for dataset in datasets:
  translate_csv(dataset, outputs[num], target_languages, num)
  num += 1

# !pip install --upgrade googletrans==4.0.0-rc1
# from googletrans import Translator
# translator = Translator()
# text = 'My name is Julia'
# translation = translator.translate(text, dest='ru')
# translation.text
