# -*- coding: utf-8 -*-
"""redo-translation-values-llms-multilingual.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11LW6f3MXhN4JENK0fBOfuIVXq_YQRq0Q
"""

# # Use a pipeline as a high-level helper
# from transformers import pipeline

# pipe = pipeline("translation", model="facebook/nllb-200-418M")

# Load model directly
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, M2M100ForConditionalGeneration


model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M")
tokenizer = AutoTokenizer.from_pretrained("facebook/m2m100_418M")

# pip install openai

# from google.colab import drive
# drive.mount('/content/drive')

#pip install paraphraser

import nltk
nltk.download('averaged_perceptron_tagger')
nltk.download('punkt')
nltk.download('wordnet')

# pip install git+https://github.com/PrithivirajDamodaran/Parrot_Paraphraser.git

pip install rouge

import pandas as pd
from nltk.translate.bleu_score import SmoothingFunction
from nltk.tokenize import word_tokenize
from nltk.corpus import wordnet
from nltk.tag import pos_tag
import time
from rouge import Rouge
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
from google.colab import files

translator = pipeline("translation", model="facebook/nllb-200-3.3B")
tokenizer = AutoTokenizer.from_pretrained("facebook/nllb-200-3.3B")
model = AutoModelForSeq2SeqLM.from_pretrained("facebook/nllb-200-3.3B")

def translate_text(text, target_language, new_lang):
    try:
        print(target_language)
        # translation = translator(text, src_lang='en', tgt_lang=target_language)
        # return translation[0]['translation_text']
        if target_language != 'en':
          tokenizer.src_lang = new_lang
        else:
          tokenizer.src_lang = 'en'
        encoded_hi = tokenizer(text, return_tensors="pt").to("cuda")
        generated_tokens = model.generate(**encoded_hi,
                                          forced_bos_token_id=tokenizer.get_lang_id(target_language))
        tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
    except Exception as e:
        print(f"Translation failed: {e}")
        return None

def calculate_rouge_score(reference, candidate):
    rouge = Rouge()
    scores = rouge.get_scores(candidate, reference)
    return scores[0]['rouge-l']['f']

def generate_paraphrases(sentence, num_paraphrases=1):
    paraphrases = set()
    tokens = word_tokenize(sentence)
    tagged_tokens = pos_tag(tokens)

    pos_groups = {}
    for token, tag in tagged_tokens:
        if tag in pos_groups:
            pos_groups[tag].append(token)
        else:
            pos_groups[tag] = [token]

    for tag, tokens_group in pos_groups.items():
        if tag.startswith('NN') or tag.startswith('VB') or tag.startswith('JJ'):
            synonyms_group = set()
            for token in tokens_group:
                for syn in wordnet.synsets(token):
                    for lemma in syn.lemmas():
                        synonym = lemma.name().replace("_", " ")
                        if lemma.name() != token and wordnet.synsets(synonym):
                            similarity_threshold = 0.7
                            if syn.path_similarity(wordnet.synsets(token)[0]) is not None and syn.path_similarity(wordnet.synsets(synonym)[0]) is not None:
                                if syn.path_similarity(wordnet.synsets(token)[0]) > similarity_threshold and syn.path_similarity(wordnet.synsets(synonym)[0]) > similarity_threshold:
                                    synonyms_group.add(synonym)

            if synonyms_group:
                for _ in range(min(num_paraphrases, len(synonyms_group))):
                    paraphrase_tokens = tokens[:]
                    for original_token, synonym in zip(tokens_group, synonyms_group):
                        for i, token in enumerate(p for p in paraphrase_tokens if p == original_token):
                            paraphrase_tokens[paraphrase_tokens.index(token)] = synonym
                    paraphrase = ' '.join(paraphrase_tokens)
                    if paraphrase != sentence:
                        paraphrases.add(paraphrase)
                    if len(paraphrases) >= num_paraphrases:
                        break
        if len(paraphrases) >= num_paraphrases:
            break
    return list(paraphrases)[:num_paraphrases]

def translate_csv(input_csv, output_csv, target_languages):
    df = pd.read_csv(input_csv)

    rows = []

    for index, row in df.iterrows():
        for lang in target_languages:
            translated_prompt = translate_text(row['English_Prompt'], lang, lang)
            back_translated_prompt = translate_text(translated_prompt, lang, lang)
            rouge_score = calculate_rouge_score(row['English_Prompt'], back_translated_prompt)
            rows.append([row['English_Prompt'], translated_prompt, back_translated_prompt, rouge_score, lang, row['Expected_Long_Term'], row['Expected_Short_Term']])

    translated_df = pd.DataFrame(rows, columns=['English_Prompt', 'Translated', 'Back_Translated', 'ROUGE_Score', 'Target_Language', 'Expected_Long_Term', 'Expected_Short_Term'])
    translated_df.to_csv(output_csv, index=False)
    files.download(output_csv)

if __name__ == "__main__":
    input_csv = "/content/long_term_vs_short_term_orientation.csv"
    output_csv = "output_longterm.csv"

    target_languages = [
        "en",  # English
        "de",  # German
        "ru",  # Russian
        "ja",  # Japanese
        "fr",  # French
        "it",  # Italian
        "zh-CN",  # Chinese
        "id",  # Indonesian
        "tr",  # Turkish
        "nl",  # Dutch
        "pl",  # Polish
        "fa",  # Persian
        "ko",  # Korean
        "cs",  # Czech
        "uk",  # Ukrainian
        "hu",  # Hungarian
        "el",  # Greek
        "ro",  # Romanian
        "sv",  # Swedish
        "he",  # Hebrew
        "da",  # Danish
        "th",  # Thai
        "fi",  # Finnish
        "bg",  # Bulgarian
        "kk",  # Kazakh
        "hy",  # Armenian
        "ka",  # Georgian
        "sq",  # Albanian
        "az",  # Azerbaijani
        "ms",  # Malay
        "mn",  # Mongolian
        "be",  # Belarusian
        "hi",  # Hindi
        "af",  # Afrikaans
        "is",  # Icelandic
        "si"   # Sinhala
    ]

    translate_csv(input_csv, output_csv, target_languages)

# !pip install --upgrade googletrans==4.0.0-rc1
# from googletrans import Translator
# translator = Translator()
# text = 'My name is Julia'
# translation = translator.translate(text, dest='ru')
# translation.text