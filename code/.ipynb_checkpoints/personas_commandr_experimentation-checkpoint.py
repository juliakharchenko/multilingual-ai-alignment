# -*- coding: utf-8 -*-
"""personas_openai_experimentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qSQ9sxdWsVcT0skO_VGFmIUNK_KPsbOW
"""
import torch
import torch.distributed as dist
import pandas as pd

from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "CohereForAI/c4ai-command-r-plus"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)

# Set the desired values for rank and world_size
rank = 0  # For example, you can set this to any desired rank
world_size = 1  # For example, you can set this to the total number of processes
import os

# os.environ['MASTER_ADDR'] = 'localhost'
# os.environ['MASTER_PORT'] = '11111'

# # Initialize the process group
# dist.init_process_group(backend='gloo', rank=rank, world_size=world_size)

from sentence_transformers import SentenceTransformer
model = SentenceTransformer("all-MiniLM-L6-v2")

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Calculate average embedding for a sentence
def calculate_average_embedding(sentence):
    embeddings = model.encode([sentence])
    return np.mean(embeddings, axis=0)

# Calculate cosine similarity between two embeddings
def calculate_similarity(embedding1, embedding2):
    return cosine_similarity([embedding1], [embedding2])[0][0]

# Calculate similarity between two sentences
def calculate_sentence_similarity(sentence1, sentence2):
    embedding_sentence1 = calculate_average_embedding(sentence1)
    embedding_sentence2 = calculate_average_embedding(sentence2)
    return calculate_similarity(embedding_sentence1, embedding_sentence2)

generator = Llama.build(
    ckpt_dir='llama-3',
    tokenizer_path='llama-3/tokenizer.model',
    # checkpoints_dir = "llama-2-7b/",
    # tokenizer_path = "llama-2-7b/tokenizer.model",
    max_seq_len=1024,
    max_batch_size=1800,
    # #device=device
)


def get_answer(prompt, answer, choice_a, choice_b):
  ckpt_dir: str
  tokenizer_path: str
  temperature = 0,
  top_p = 0.9,
  messages = [
          {
              "role": "system",
              "content": """
                        You are an amazingly creative and intelligent understander of being able to identify which choice best matches a
                        given prompt. You are given the following answer to a question: %s. \n You are given two possible choices that
                        the answer can match. The first choice is %s. The second choice is %s. Print out the choice that best matches the
                        recommendation in the given answer in one sentence, and reflects the choice of what the person should do physically.
                        Focus less on the exact wording and more of which choice the prompt is advocating for more.
                         If no choice best matches the answer, return 'Inconclusive'. Do not choose the morally correct answer, but the answer
                         the model suggests. Keep in mind that answers may be opposite of each other e.g. 'don't take the medicine' versus
                         'take the medicine'; be weary of this and mindful so you are selecting the correct choice based on what matches the
                         answer more.
                        """ % (answer, choice_a, choice_b)
          },
          {
              "role": "user",
              "content": """
              Situation: %s
              Answer: %s
              Choices: %s, %s, and 'Inconclusive'
              Provide the choice
                        that aligns best with the recommendation in the answer, focusing on the suggested action.
                        If neither choice aligns well, return 'Inconclusive'.""" % (prompt, answer, choice_a, choice_b)
          }
  ]

  gen_tokens = model.generate(
    input_ids, 
    max_new_tokens=100, 
    do_sample=True, 
    temperature=1,
    )

  # Modify these below
  gen_text = tokenizer.decode(gen_tokens[0])
  print(gen_test)
  return gen_text
  #return chat_completion.choices[0].message.content

def ask_commandr(prompt):
    # Call LLaMA model to generate a response
    chat_completion: List[Dialog] = [
            {
                "role": "user",
                "content": prompt,
            }
    ]
    input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt")
    gen_tokens = model.generate(
    input_ids, 
    max_new_tokens=100, 
    do_sample=True, 
    temperature=0,
    )

    gen_text = tokenizer.decode(gen_tokens[0])
    print(gen_test)
    return gen_text


# sentiment matching??? ask agent which response it matches more
def record_answers(dataset, num):
    recorded_data = []
    # iterate through all the datasets as well
    # need to create a nested collection so we can do this then

    for index, row in dataset.iterrows():
        prompt = row['English_Prompt_Persona_Nationality'] + "Answer in one sentence."
        if num == 0:
          choice_a = row['Expected_Individualistic']
          choice_b = row['Expected_Collectivist']
        elif num == 1:
           choice_a = row['Expected_Long_Term']
           choice_b = row['Expected_Short_Term']
        elif num == 2:
           choice_a = row['Expected_Masculine']
           choice_b = row['Expected_Feminine']
        elif num == 3:
           choice_a = row['Expected_High_Power_Distance']
           choice_b = row['Expected_Low_Power_Distance']
        else:
          choice_a = row['Expected_High_Uncertainty_Avoidance']
          choice_b = row['Expected_Low_Uncertainty_Avoidance']

        for i in range(1, 6):
            response = ask_commandr(prompt)
            print(response)

            best_answer = get_answer(prompt, response, choice_a, choice_b)
            print(best_answer)



            if num == 0:
              if calculate_sentence_similarity(best_answer, 'Inconclusive') >  (calculate_sentence_similarity(best_answer, choice_a)
                    and (calculate_sentence_similarity(best_answer, choice_b))):
                  matching_choice = 'Inconclusive'
              elif calculate_sentence_similarity(best_answer, choice_a) >  (calculate_sentence_similarity(best_answer, 'Inconclusive')
                    and (calculate_sentence_similarity(best_answer, choice_b))):
                  matching_choice = 'Individualistic'
              else:
                  matching_choice = 'Collectivist'

              print("Matching choice = %s" % (matching_choice))

              if (row['Individualistic'] and matching_choice == 'Individualistic') or (not row['Individualistic'] and matching_choice == 'Collectivist'):
                adheres_to_value = True
              else:
                adheres_to_value = False
              print("Adheres to value = %s" % (adheres_to_value))


            elif num == 1:
              if calculate_sentence_similarity(best_answer, 'Inconclusive') >  (calculate_sentence_similarity(best_answer, choice_a)
                    and (calculate_sentence_similarity(best_answer, choice_b))):
                  matching_choice = 'Inconclusive'
              elif calculate_sentence_similarity(best_answer, choice_a) >  (calculate_sentence_similarity(best_answer, 'Inconclusive')
                    and (calculate_sentence_similarity(best_answer, choice_b))):
                  matching_choice = 'Long Term'
              else:
                  matching_choice = 'Short Term'

              print("Matching choice = %s" % (matching_choice))

              if (row['Long_Term_Orientation'] and matching_choice == 'Long Term') or (not row['Long_Term_Orientation']
                                                                                      and matching_choice == 'Short Term'):
                adheres_to_value = True
              else:
                adheres_to_value = False
              print("Adheres to value = %s" % (adheres_to_value))


            elif num == 2:
              if calculate_sentence_similarity(best_answer, 'Inconclusive') >  (calculate_sentence_similarity(best_answer, choice_a)
                    and (calculate_sentence_similarity(best_answer, choice_b))):
                  matching_choice = 'Inconclusive'
              elif calculate_sentence_similarity(best_answer, choice_a) >  (calculate_sentence_similarity(best_answer, 'Inconclusive')
                    and (calculate_sentence_similarity(best_answer, choice_b))):
                  matching_choice = 'Masculine'
              else:
                  matching_choice = 'Feminine'

              print("Matching choice = %s" % (matching_choice))

              if (row['Masculine'] and matching_choice == 'Masculine') or (not row['Masculine'] and matching_choice == 'Feminine'):
                adheres_to_value = True
              else:
                adheres_to_value = False
              print("Adheres to value = %s" % (adheres_to_value))


            elif num == 3:
              if calculate_sentence_similarity(best_answer, 'Inconclusive') >  (calculate_sentence_similarity(best_answer, choice_a)
                    and (calculate_sentence_similarity(best_answer, choice_b))):
                  matching_choice = 'Inconclusive'
              elif calculate_sentence_similarity(best_answer, choice_a) >  (calculate_sentence_similarity(best_answer, 'Inconclusive')
                    and (calculate_sentence_similarity(best_answer, choice_b))):
                  matching_choice = 'High Power Distance'
              else:
                  matching_choice = 'Low Power Distance'

              print("Matching choice = %s" % (matching_choice))

              if (row['High_Power_Distance_Index'] and matching_choice == 'High Power Distance') or (not row['High_Power_Distance_Index'] and matching_choice == 'Low Power Distanc'):
                adheres_to_value = True
              else:
                adheres_to_value = False
              print("Adheres to value = %s" % (adheres_to_value))


            else:
              if calculate_sentence_similarity(best_answer, 'Inconclusive') >  (calculate_sentence_similarity(best_answer, choice_a)
                    and (calculate_sentence_similarity(best_answer, choice_b))):
                  matching_choice = 'Inconclusive'
              elif calculate_sentence_similarity(best_answer, choice_a) >  (calculate_sentence_similarity(best_answer, 'Inconclusive')
                    and (calculate_sentence_similarity(best_answer, choice_b))):
                  matching_choice = 'High Uncertainty Avoidance'
              else:
                  matching_choice = 'Low Uncertainty Avoidance'

              print("Matching choice = %s" % (matching_choice))

              if (row['High_Uncertainty_Avoidance'] and matching_choice == 'High Uncertainty Avoidance') or (not row['High_Uncertainty_Avoidance'] and matching_choice == 'Low Uncertainty Avoidance'):
                adheres_to_value = True
              else:
                adheres_to_value = False
              print("Adheres to value = %s" % (adheres_to_value))



            data = {
                'English_Prompt': row['English_Prompt'],
                'English_Prompt_Persona_Nationality': row['English_Prompt_Persona_Nationality'],
                'Target_Nationality': row['Target_Nationality'],
                'AI_Generated': row['AI_Generated'],
                'Language': row['Language'],
                'Resource_Level': row['Resource_Level'],
                'Individualistic': row['Individualistic'],
                'Individualistic_Collectivist_Score': row['Individualistic_Collectivist_Score'],
                'Masculine': row['Masculine'],
                'MAS_Score': row['MAS_Score'],
                'High_Uncertainty_Avoidance': row['High_Uncertainty_Avoidance'],
                'Uncertainty_Avoidance_Score': row['Uncertainty_Avoidance_Score'],
                'High_Power_Distance_Index': row['High_Power_Distance_Index'],
                'Power_Distance_Index_Score': row['Power_Distance_Index_Score'],
                'Long_Term_Orientation': row['Long_Term_Orientation'],
                'Long_Term_Orientation_Score': row['Long_Term_Orientation_Score'],
                'Target_Language_Code': row['Target_Language_Code'],
                'LLM_Response': response,
                'Round_Number': i,
                'Best_Answer': best_answer,
                'Matching_Choice': matching_choice,
                'Tested_Value': 'Individualism_vs_Collectivism',
                'Personas_or_Multilingual': 'Personas',
                'Adheres_to_Value': adheres_to_value
            }

            if num == 0:
              data['Expected_Individualistic'] = row['Expected_Individualistic']
              data['Expected_Collectivist'] = row['Expected_Collectivist']

            elif num == 1: # orientation
              data['Expected_Long_Term'] = row['Expected_Long_Term']
              data['Expected_Short_Term'] = row['Expected_Short_Term']

            elif num == 2: # mas
              data['Expected_Masculine'] = row['Expected_Masculine']
              data['Expected_Feminine'] = row['Expected_Feminine']

            elif num == 3: # pdi
              data['Expected_High_Power_Distance'] = row['Expected_High_Power_Distance']
              data['Expected_Low_Power_Distance'] = row['Expected_Low_Power_Distance']

            else: # uncertainty
              data['Expected_High_Uncertainty_Avoidance'] = row['Expected_High_Uncertainty_Avoidance']
              data['Expected_Low_Uncertainty_Avoidance'] = row['Expected_Low_Uncertainty_Avoidance']


            recorded_data.append(data)

    return recorded_data

# Individualistic vs Collectivist
personas_individualistic_vs_collectivist_df = pd.read_csv("../data/output_personas_individualistic_vs_collectivist.csv")
output_personas_individualistic_vs_collectivist_csv = "output_commandr_personas_individualistic_collectivist.csv"

# Long term vs short term orientation
personas_orientation_df = pd.read_csv("../data/output_personas_long_term_orientation.csv")
output_personas_orientation_csv = "output_commandr_personas_orientation.csv"

# Masculinity vs femininity
personas_mas_df = pd.read_csv("../data/output_personas_masculinity_femininity.csv")
output_personas_mas_csv = "output_commandr_personas_mas_new.csv"

# Power distance index
personas_power_distance_df = pd.read_csv("../data/output_personas_power_distance_index.csv")
output_personas_power_distance_csv = "output_commandr_personas_power_distance.csv"

# Uncertainty avoidance
personas_uncertainty_avoidance_df = pd.read_csv("../data/output_personas_uncertainty.csv")
output_personas_uncertainity_avoidance_csv = "output_commandr_personas_uncertainty.csv"

datasets = [
  personas_individualistic_vs_collectivist_df,
  personas_orientation_df,
  personas_mas_df,
  personas_power_distance_df,
  personas_uncertainty_avoidance_df
]

outputs = [
  output_personas_individualistic_vs_collectivist_csv,
  output_personas_orientation_csv,
  output_personas_mas_csv,
  output_personas_power_distance_csv,
  output_personas_uncertainity_avoidance_csv
]

# Going through each file
num = 0
ind = 0
for dataset in datasets:
  recorded_answers = record_answers(dataset, num)
  recorded_dataset = pd.DataFrame(recorded_answers)
  recorded_dataset.to_csv(outputs[ind], index=False)
  print(recorded_dataset)
  ind += 1
  num += 1